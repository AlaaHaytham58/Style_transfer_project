{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60338c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn_style_transfer_with_mask.py\n",
    "\n",
    "A Gatys-style neural style transfer script extended to mimic the functionality described in:\n",
    "\"Style-Transfer via Texture-Synthesis\" (Elad & Milanfar, 2016) — multi-scale, palette matching,\n",
    "and segmentation mask W to preserve content regions.\n",
    "\n",
    "Usage:\n",
    "    python nn_style_transfer_with_mask.py \n",
    "        --content content.jpg \n",
    "        --style style.jpg \n",
    "        --mask face_mask.png   # optional (grayscale, same size as content). White=preserve\n",
    "        --out result.jpg\n",
    "\n",
    "Dependencies:\n",
    "    pip install torch torchvision pillow numpy scikit-image\n",
    "\"\"\"\n",
    "#Colab for GPU: https://colab.research.google.com/drive/1bHCWFXnwWDOGI3knMbplxyGr9Dxocax3?usp=sharing\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from skimage.exposure import match_histograms\n",
    "import os\n",
    " \n",
    " \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19cea022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "def load_image(path, target_size=None):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    if target_size is not None:\n",
    "        img = img.resize(target_size, Image.LANCZOS) # خوارزمية إعادة التحجيم (resampling) تعطي نتائج ناعمة ودقيقة\n",
    "    return img\n",
    "\n",
    "#تحويل من صيغة بايثون للثور لتنسور\n",
    "def pil_to_tensor(img):\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    return transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "def tensor_to_pil(tensor):\n",
    "    t = tensor.detach().cpu().squeeze(0).clamp(0,255).div(255)\n",
    "    transform = T.ToPILImage()\n",
    "    return transform(t)\n",
    "\n",
    "\n",
    "# نُسطّح (flatten) الأبعاد المكانية  \n",
    "# [C,H×W]\n",
    "# نحسب مصفوفة غرام بضرب المصفوفة في محورها المنقول:\n",
    "# G=F⋅FT\n",
    "# النتيجة: مصفوفة بُعدها \n",
    "# [C,C]، كل عنصر فيها يُظهر مدى ارتباط قناة معينة بأخرى عبر كل موضع في الصورة.\n",
    "def gram_matrix(features):\n",
    "    (b, ch, h, w) = features.size()\n",
    "    features = features.view(b, ch, h*w)\n",
    "    G = torch.bmm(features, features.transpose(1,2))\n",
    "    return G / (ch * h * w)\n",
    "\n",
    "def total_variation_loss(img):\n",
    "    # كلما كانت الصورة أكثر سلاسة (مثل لوحة زيتية)، قلّت هذه القيمة.\n",
    "    # Horizontal differences (along width)\n",
    "    x_diff = img[..., :, 1:] - img[..., :, :-1]\n",
    "    # Vertical differences (along height)\n",
    "    y_diff = img[..., 1:, :] - img[..., :-1, :]  \n",
    "    return (x_diff.abs().mean() + y_diff.abs().mean())\n",
    "\n",
    "# Helper: resize tensor image (1,3,H,W) to PIL-size (H,W) keeping [0..255]\n",
    "def tensor_resize(tensor, size):\n",
    "    # size is (H, W) (note PIL uses width,height but we pass height,width)\n",
    "    t = tensor.detach().cpu().clamp(0,255).squeeze(0) / 255.0\n",
    "    transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize((size[0], size[1]), Image.LANCZOS),\n",
    "        T.ToTensor(),\n",
    "        T.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    return transform(t).unsqueeze(0).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474fa1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "        self.selected_layers = layers\n",
    "        self.layers = vgg\n",
    "        # freeze params\n",
    "        for p in self.layers.parameters():\n",
    "            p.requires_grad = False     #لا تحديث أوزان VGG\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = {}\n",
    "        for i, l in enumerate(self.layers):\n",
    "            x = l(x)\n",
    "            name = f\"l{i}\"\n",
    "            if name in self.selected_layers:\n",
    "                results[name] = x\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7640a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gatys Neural Style Transfer (coarse-to-fine)\n",
    "def run_style_transfer(content_img_pil, style_img_pil,\n",
    "                       out_size=(400,400), num_steps=300, style_weight=1e6,\n",
    "                       content_weight=1e0, tv_weight=1e-6, lr=0.02,\n",
    "                       pyramid_scales=[(100,100),(200,200),(400,400)]):\n",
    "    \"\"\"\n",
    "    Performs coarse-to-fine style transfer.\n",
    "    - mask_pil: grayscale PIL image same size as content. White=preserve content (W high), black=free to stylize.\n",
    "    \"\"\"\n",
    "    # Prepare VGG extractor layers (common choices)\n",
    "    # We'll use conv layers indices as names: l0..lN (matching VGG19 features)\n",
    "    #طبقاته المبكرة تلتقط تفاصيل بسيطة (حواف، ألوان، نقوش)\n",
    "    #طبقاته العميقة تلتقط مفاهيم معقدة (كائنات، هياكل عالية المستوى).\n",
    "    style_layers = ['l1','l6','l11','l20']   # shallow->deep conv layers for style Gram\n",
    "    content_layer = 'l21'                   # deeper layer for content\n",
    "    all_layers = style_layers + [content_layer]\n",
    "\n",
    "    extractor = VGGFeatures(all_layers).to(device)\n",
    "\n",
    "    # convert style image to tensor and precompute style grams per scale\n",
    "    style_pil_original = style_img_pil.copy()\n",
    "    content_pil_original = content_img_pil.copy()\n",
    "   \n",
    "    # initial image X: we will start with content + noise as in the paper\n",
    "    X = pil_to_tensor(content_pil_original).clone()\n",
    "    noise = torch.randn_like(X) * 50.0  # sigma=50 as in paper initialization\n",
    "    #requires_grad_(True) يجعل هذا الـ tensor قابلًا للتفاضل — أي أن PyTorch سيحسب التدرجات بالنسبة له أثناء الـ backward()\n",
    "    X = (X + noise).clamp(0,255).detach().requires_grad_(True).to(device)\n",
    "\n",
    "    # iterate scales from coarse->fine\n",
    "    scales = pyramid_scales\n",
    "    for (w,h) in scales:\n",
    "        print(f\"\\n--- Scale {(w,h)} ---\")\n",
    "        # resize images\n",
    "        Cp = content_pil_original.resize((w,h), Image.LANCZOS)\n",
    "        Sp = style_pil_original.resize((w,h), Image.LANCZOS)\n",
    "        Sp_tensor = pil_to_tensor(Sp)\n",
    "\n",
    "        # palette matching: match content to style palette before optimization at each scale\n",
    "        # we perform histogram matching using skimage (works on numpy)\n",
    "        content_np = np.array(Cp).astype(np.uint8)\n",
    "        style_np = np.array(Sp).astype(np.uint8)\n",
    "        # تُعدّل توزيع ألوان صورة المحتوى لتشبه توزيع ألوان صورة النمط\n",
    "        matched = match_histograms(content_np, style_np, channel_axis=-1).astype(np.uint8)\n",
    "        Cp_matched_pil = Image.fromarray(matched)\n",
    "        Cp_tensor_matched = pil_to_tensor(Cp_matched_pil)\n",
    "\n",
    "        # compute style grams\n",
    "        with torch.no_grad():\n",
    "            sp_feats = extractor(Sp_tensor/255.0)\n",
    "            style_grams = {k: gram_matrix(v) for k,v in sp_feats.items() if k in style_layers}\n",
    "\n",
    "        # resize current X to scale\n",
    "        X = tensor_resize(X, (h,w))  # helper below\n",
    "        X = X.detach().requires_grad_(True)\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = optim.LBFGS([X], lr=lr)  # LBFGS is classic for style transfer\n",
    "        run = [0]\n",
    "        # Optimize the generated image X for the current scale using LBFGS.\n",
    "        # We use a closure because LBFGS requires a re-evaluatable function for line search.\n",
    "        while run[0] < num_steps:\n",
    "            def closure():\n",
    "                # Zero gradients from the previous step\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Normalize input to [0, 1] as expected by the pretrained VGG model\n",
    "                X_norm = X / 255.0\n",
    "                feats = extractor(X_norm)  # Extract feature maps from selected VGG layers\n",
    "                \n",
    "                # --- Content Loss ---\n",
    "                # Get features of the current generated image at the content layer\n",
    "                content_feat = feats[content_layer]\n",
    "                # Get target content features from the (color-matched) content image (detached to avoid gradients)\n",
    "                target_content = extractor(Cp_tensor_matched / 255.0)[content_layer].detach()\n",
    "                # Compute Mean Squared Error between generated and target content features\n",
    "                c_loss = nn.MSELoss()(content_feat, target_content)\n",
    "\n",
    "                # --- Style Loss ---\n",
    "                # Accumulate style loss across all selected style layers using Gram matrices\n",
    "                s_loss = 0.0\n",
    "                for l in style_layers:\n",
    "                    G = gram_matrix(feats[l])        # Gram matrix of current generated image\n",
    "                    A = style_grams[l]               # Precomputed Gram matrix of the style image\n",
    "                    s_loss = s_loss + nn.MSELoss()(G, A.expand_as(G))  # Match style representations\n",
    "\n",
    "                # --- Total Variation (TV) Loss ---\n",
    "                # Encourage spatial smoothness in the output image (reduce high-frequency noise)\n",
    "                tv = total_variation_loss(X)\n",
    "\n",
    "                # --- Total Loss ---\n",
    "                # Weighted combination of content, style, and TV losses\n",
    "                loss = content_weight * c_loss + style_weight * s_loss + tv_weight * tv\n",
    "                \n",
    "                # Backpropagate gradients through the generated image X\n",
    "                loss.backward()\n",
    "                \n",
    "                # Log losses every 50 iterations for monitoring\n",
    "                if run[0] % 50 == 0:\n",
    "                    print(f\"iter {run[0]} loss: content {c_loss.item():.4e} style {s_loss.item():.4e} tv {tv.item():.4e}\")\n",
    "                \n",
    "                # Increment the step counter (using list to allow mutation inside closure)\n",
    "                run[0] += 1\n",
    "                return loss  # LBFGS needs the loss value returned by closure\n",
    "            \n",
    "            # Perform one optimization step (LBFGS may call closure multiple times internally)\n",
    "            optimizer.step(closure)\n",
    "            # نمرّر X عبر VGG → نأخذ ميزاته في طبقة عميقة (مثل l21).\n",
    "            # نمرّر صورة المحتوى عبر VGG → نأخذ ميزاته في نفس الطبقة.\n",
    "            # نحسب MSE بين الميزتين:\n",
    "            # هذا يقيس: \"هل الهيكل العام لـ X مشابه للمحتوى؟\"\n",
    "            # ب. خسارة النمط:\n",
    "            # نحسب مصفوفة غرام لـ X من طبقات متعددة.\n",
    "            # نحسب مصفوفة غرام لصورة النمط من نفس الطبقات.\n",
    "            # نحسب MSE بين كل زوج\n",
    "\n",
    "        # --- Post-optimization color refinement ---\n",
    "        # After optimizing at this scale, re-match the color palette of the output to the style image\n",
    "        # to maintain consistent colors before proceeding to the next (finer) scale.\n",
    "        X_img = tensor_to_pil(X)  # Convert tensor back to PIL image\n",
    "        X_np = np.array(X_img).astype(np.uint8)\n",
    "        Sp_np = np.array(Sp)      # Style image at current scale as NumPy array\n",
    "        # Apply histogram matching to align color distributions\n",
    "        X_matched = match_histograms(X_np, Sp_np, channel_axis=-1).astype(np.uint8)\n",
    "        # Convert back to tensor and re-enable gradients for the next scale optimization\n",
    "        X = pil_to_tensor(Image.fromarray(X_matched)).detach().requires_grad_(True).to(device)\n",
    "        \n",
    "    # final result (resize to requested out_size)\n",
    "    final = tensor_resize(X, (out_size[1], out_size[0]))\n",
    "    return tensor_to_pil(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0230aa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\p c\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\p c\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scale (100, 100) ---\n",
      "iter 0 loss: content 2.5637e+00 style 3.4549e-05 tv 3.3182e+01\n",
      "iter 50 loss: content 2.5553e+00 style 1.8787e-05 tv 3.3614e+01\n",
      "iter 100 loss: content 3.1608e+00 style 1.1014e-05 tv 3.5820e+01\n",
      "iter 150 loss: content 3.4340e+00 style 8.1848e-06 tv 3.8047e+01\n",
      "\n",
      "--- Scale (200, 200) ---\n",
      "iter 0 loss: content 3.1148e+00 style 2.3348e-05 tv 2.1710e+01\n",
      "iter 50 loss: content 3.1148e+00 style 2.3348e-05 tv 2.1710e+01\n",
      "iter 100 loss: content 3.1148e+00 style 2.3348e-05 tv 2.1710e+01\n",
      "\n",
      "--- Scale (400, 400) ---\n",
      "iter 0 loss: content 2.1720e+00 style 1.5054e-05 tv 1.2334e+01\n",
      "iter 50 loss: content 2.1720e+00 style 1.5054e-05 tv 1.2334e+01\n",
      "iter 100 loss: content 2.1720e+00 style 1.5054e-05 tv 1.2334e+01\n",
      "Saved: model_result.jpg\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    C = load_image(\"../Data/content/wave_home.jpg\")\n",
    "    S = load_image(\"../Data/style/waves.jpg\")\n",
    "    output='model_result.jpg'\n",
    "\n",
    "    res = run_style_transfer(C, S, num_steps=150 )\n",
    "\n",
    "    res.save(output)\n",
    "    print(\"Saved:\", output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
